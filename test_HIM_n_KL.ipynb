{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test HIM and KLqp in Edward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2018.1.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset iris with features [-1,1] normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizer(array):\n",
    "    \"\"\"normalize the array(axis=0) to [head, tail]\"\"\"\n",
    "    amin, amax = array.min(axis=0), array.max(axis=0)\n",
    "    return ((np.subtract(array, amin) / (amax - amin))*2-1.)\n",
    "\n",
    "def generator(arrays, batch_size):\n",
    "    \"\"\"Generate batches, one with respect to each array's first axis.\"\"\"\n",
    "    starts = [0] * len(arrays)    # pointers to where we are in iteration\n",
    "    while True:\n",
    "        batches = []\n",
    "        for i, array in enumerate(arrays):\n",
    "            start = starts[i]\n",
    "            stop = start + batch_size\n",
    "            diff = stop - array.shape[0]\n",
    "            if diff <= 0:\n",
    "                batch = array[start:stop]\n",
    "                starts[i] += batch_size\n",
    "            else:\n",
    "                batch = np.concatenate((array[start:], array[:diff]))\n",
    "                starts[i] = diff\n",
    "            batches.append(batch)\n",
    "        yield batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 20    # batch size during training\n",
    "D = 4    # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "Value = normalizer(iris.data)\n",
    "Label = iris.target.reshape([-1,1])\n",
    "DataXy = np.concatenate([Value, Label],1)\n",
    "np.random.shuffle(DataXy)\n",
    "X_train = DataXy[:100,:4]\n",
    "y_train = DataXy[:100,-1]\n",
    "X_test = DataXy[100:,:4]\n",
    "y_test = DataXy[100:,-1]\n",
    "data = generator([X_train, y_train], M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn linear: \n",
      " train mse: 0.070231\t test mse: 0.074968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenjunhua/anaconda2/lib/python2.7/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# sklearn linear model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_trn = regr.predict(X_train)\n",
    "y_tst = regr.predict(X_test)\n",
    "print('sklearn linear: \\n train mse: %f\\t test mse: %f' % \n",
    "      (mean_squared_error(y_train, y_trn), mean_squared_error(y_test, y_tst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "X = tf.placeholder(tf.float32, [None, D])\n",
    "y_ph = tf.placeholder(tf.float32, [None])\n",
    "w = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "y = Normal(loc=ed.dot(X, w), scale=tf.ones(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference HIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ratio_estimator(data, local_vars, global_vars):\n",
    "    \"\"\"Takes as input a dict of data x, local variable samples z, and\n",
    "    global variable samples beta; outputs real values of shape\n",
    "    (x.shape[0] + z.shape[0],). In this example, there are no local\n",
    "    variables.\n",
    "    \"\"\"\n",
    "    # data[y] has shape (M,); global_vars[w] has shape (D,)\n",
    "    # we concatenate w to each data point y, so input has shape (M, 1 + D)\n",
    "    input = tf.concat([\n",
    "            tf.reshape(data[y], [M, 1]),\n",
    "            tf.tile(tf.reshape(global_vars[w], [1, D]), [M, 1])], 1)\n",
    "    hidden = slim.fully_connected(input, 64, activation_fn=tf.nn.leaky_relu)\n",
    "    output = slim.fully_connected(hidden, 1, activation_fn=None)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/5000 [  0%]                                ETA: 5515s | Disc Loss: 1.027 | Gen Loss: 22.461\n",
      "Inferred mean & std:\n",
      "[-0.10964653 -0.21976471  0.62489611 -0.46815526]\n",
      "[ 0.31523529  0.89373672  0.15893783  0.83554077]\n",
      "#### mse: 1.436791\ttest mse: 1.800727\n",
      "\n",
      " 500/5000 [ 10%] ███                            ETA: 43s | Disc Loss: 0.911 | Gen Loss: 22.734\n",
      "Inferred mean & std:\n",
      "[ 1.14194131  0.46966857  0.32792512  0.29924989]\n",
      "[ 0.46790308  0.74713898  0.86422336  0.30036217]\n",
      "#### mse: 1.372312\ttest mse: 1.839772\n",
      "\n",
      "1000/5000 [ 20%] ██████                         ETA: 33s | Disc Loss: 1.001 | Gen Loss: 17.665\n",
      "Inferred mean & std:\n",
      "[-0.02411987 -0.25009912 -0.18093771 -0.66380525]\n",
      "[ 0.32037061  0.69662017  0.71519917  0.29345939]\n",
      "#### mse: 1.678828\ttest mse: 1.766991\n",
      "\n",
      "1500/5000 [ 30%] █████████                      ETA: 28s | Disc Loss: 0.758 | Gen Loss: 18.227\n",
      "Inferred mean & std:\n",
      "[ 0.08030156  0.02773621  0.00449938 -0.68941444]\n",
      "[ 0.31363761  0.60558254  0.71963865  0.25051531]\n",
      "#### mse: 1.438620\ttest mse: 2.303640\n",
      "\n",
      "2000/5000 [ 40%] ████████████                   ETA: 24s | Disc Loss: 0.819 | Gen Loss: 19.692\n",
      "Inferred mean & std:\n",
      "[ 0.18346313 -0.0697314   0.10718558 -0.84657061]\n",
      "[ 0.26441327  0.63816011  0.62538379  0.2611011 ]\n",
      "#### mse: 1.490548\ttest mse: 2.013763\n",
      "\n",
      "2500/5000 [ 50%] ███████████████                ETA: 19s | Disc Loss: 0.969 | Gen Loss: 19.816\n",
      "Inferred mean & std:\n",
      "[ 0.05382541  0.0297245   0.06977865 -0.72483343]\n",
      "[ 0.29443532  0.60800487  0.66323191  0.26742148]\n",
      "#### mse: 1.394853\ttest mse: 2.054026\n",
      "\n",
      "3000/5000 [ 60%] ██████████████████             ETA: 15s | Disc Loss: 0.920 | Gen Loss: 18.869\n",
      "Inferred mean & std:\n",
      "[ 0.11276226 -0.03365735  0.06003653 -0.77927852]\n",
      "[ 0.26637638  0.58143008  0.62433302  0.25838795]\n",
      "#### mse: 1.550179\ttest mse: 2.210371\n",
      "\n",
      "3500/5000 [ 70%] █████████████████████          ETA: 11s | Disc Loss: 0.769 | Gen Loss: 18.559\n",
      "Inferred mean & std:\n",
      "[ 0.1212272  -0.0301536   0.06805082 -0.78787851]\n",
      "[ 0.26582101  0.58937812  0.64512962  0.25271699]\n",
      "#### mse: 1.562946\ttest mse: 1.969825\n",
      "\n",
      "4000/5000 [ 80%] ████████████████████████       ETA: 7s | Disc Loss: 0.898 | Gen Loss: 20.247\n",
      "Inferred mean & std:\n",
      "[ 0.1197497  -0.02855209  0.06544797 -0.78731138]\n",
      "[ 0.27010134  0.61317503  0.62414211  0.25057119]\n",
      "#### mse: 1.452569\ttest mse: 1.752678\n",
      "\n",
      "4500/5000 [ 90%] ███████████████████████████    ETA: 3s | Disc Loss: 0.961 | Gen Loss: 18.789\n",
      "Inferred mean & std:\n",
      "[ 0.11638752 -0.01771249  0.04706962 -0.78030914]\n",
      "[ 0.27739832  0.60322356  0.64697099  0.25130272]\n",
      "#### mse: 1.713751\ttest mse: 1.502022\n",
      "\n",
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 39s | Disc Loss: 0.950 | Gen Loss: 19.452\n",
      "\n",
      "Inferred mean & std:\n",
      "[ 0.13540252 -0.03404944  0.05958608 -0.80046374]\n",
      "[ 0.28876504  0.59957463  0.66312134  0.25332502]\n",
      "#### mse: 1.652404\ttest mse: 1.985687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "\n",
    "qw = Normal(loc=tf.Variable(tf.random_normal([D]) + .0),\n",
    "                        scale=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "\n",
    "inference = ed.ImplicitKLqp(\n",
    "        {w: qw}, data={y: y_ph},\n",
    "        discriminator=ratio_estimator, global_vars={w: qw})\n",
    "inference.initialize(n_iter=5000, n_print=500)\n",
    "\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "for _ in range(inference.n_iter):\n",
    "    X_batch, y_batch = next(data)\n",
    "    for _ in range(5):\n",
    "        info_dict_d = inference.update(\n",
    "                variables=\"Disc\", feed_dict={X: X_batch, y_ph: y_batch})\n",
    "\n",
    "    info_dict = inference.update(\n",
    "            variables=\"Gen\", feed_dict={X: X_batch, y_ph: y_batch})\n",
    "    info_dict['loss_d'] = info_dict_d['loss_d']\n",
    "    info_dict['t'] = info_dict['t'] // 6    # say set of 6 updates is 1 iteration\n",
    "\n",
    "    t = info_dict['t']\n",
    "    inference.print_progress(info_dict)\n",
    "    if t == 1 or t % inference.n_print == 0:\n",
    "        # Check inferred posterior parameters.\n",
    "        mean, std = sess.run([qw.mean(), qw.stddev()])\n",
    "        print(\"\\nInferred mean & std:\")\n",
    "        print(mean)\n",
    "        print(std)\n",
    "        print('#### mse: %f\\ttest mse: %f\\n' % \n",
    "              (ed.evaluate('mean_squared_error', data={X: X_train, y: y_train}, n_samples=500),\n",
    "               ed.evaluate('mean_squared_error', data={X: X_test, y: y_test}, n_samples=500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference KLqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/5000 [  0%]                                ETA: 4733s | Loss: 218.260\n",
      "Inferred mean & std:\n",
      "[-0.12962687 -1.69752562  0.24212778  0.45240113]\n",
      "[ 1.58082426  0.38093698  1.65014386  0.21955991]\n",
      "#### mse: 1.421185\ttest mse: 1.723321\n",
      "\n",
      "\n",
      "Inferred mean & std:\n",
      "[-0.09637856 -1.70728397  0.24871224  0.41900766]\n",
      "[ 1.50213301  0.36035776  1.57123733  0.23764306]\n",
      "#### mse: 1.417322\ttest mse: 2.017613\n",
      "\n",
      " 500/5000 [ 10%] ███                            ETA: 51s | Loss: 102.681\n",
      "Inferred mean & std:\n",
      "[ 0.44218385 -0.83457851  0.91158158 -0.31776103]\n",
      "[ 0.10731237  0.21839711  0.19535615  0.07223265]\n",
      "#### mse: 1.612435\ttest mse: 1.990198\n",
      "\n",
      "1000/5000 [ 20%] ██████                         ETA: 31s | Loss: 106.589\n",
      "Inferred mean & std:\n",
      "[ 0.41305426 -0.84566212  0.9213053  -0.36596432]\n",
      "[ 0.10845062  0.17922792  0.18315829  0.09811689]\n",
      "#### mse: 1.726522\ttest mse: 1.875740\n",
      "\n",
      "1500/5000 [ 30%] █████████                      ETA: 23s | Loss: 105.668\n",
      "Inferred mean & std:\n",
      "[ 0.3568694  -0.85863274  0.94876331 -0.33341444]\n",
      "[ 0.09535474  0.24938987  0.22513683  0.09806394]\n",
      "#### mse: 1.682066\ttest mse: 1.685100\n",
      "\n",
      "2000/5000 [ 40%] ████████████                   ETA: 18s | Loss: 103.967\n",
      "Inferred mean & std:\n",
      "[ 0.38766614 -0.88060397  0.97826201 -0.3728666 ]\n",
      "[ 0.09495316  0.20490418  0.19709149  0.09317614]\n",
      "#### mse: 1.604931\ttest mse: 1.922468\n",
      "\n",
      "2500/5000 [ 50%] ███████████████                ETA: 14s | Loss: 102.589\n",
      "Inferred mean & std:\n",
      "[ 0.35978627 -0.82835293  0.9132359  -0.34874406]\n",
      "[ 0.09560742  0.19960904  0.20885094  0.11103404]\n",
      "#### mse: 1.601874\ttest mse: 1.837410\n",
      "\n",
      "3000/5000 [ 60%] ██████████████████             ETA: 11s | Loss: 103.732\n",
      "Inferred mean & std:\n",
      "[ 0.36323082 -0.8195045   0.90189838 -0.35402837]\n",
      "[ 0.09323576  0.20653112  0.20701961  0.09998634]\n",
      "#### mse: 1.406952\ttest mse: 1.896559\n",
      "\n",
      "3500/5000 [ 70%] █████████████████████          ETA: 8s | Loss: 103.817\n",
      "Inferred mean & std:\n",
      "[ 0.34856689 -0.83597213  0.92068756 -0.34032819]\n",
      "[ 0.09696541  0.20356086  0.21437913  0.10010499]\n",
      "#### mse: 1.557249\ttest mse: 2.186724\n",
      "\n",
      "4000/5000 [ 80%] ████████████████████████       ETA: 5s | Loss: 102.527\n",
      "Inferred mean & std:\n",
      "[ 0.35087809 -0.82424837  0.90384871 -0.34316954]\n",
      "[ 0.09614713  0.20342334  0.20905688  0.09426007]\n",
      "#### mse: 1.347536\ttest mse: 1.842036\n",
      "\n",
      "4500/5000 [ 90%] ███████████████████████████    ETA: 2s | Loss: 102.630\n",
      "Inferred mean & std:\n",
      "[ 0.35025245 -0.82439095  0.90420121 -0.34284696]\n",
      "[ 0.09744427  0.20729803  0.21504265  0.09851007]\n",
      "#### mse: 1.608508\ttest mse: 1.649869\n",
      "\n",
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 26s | Loss: 103.445\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE v2\n",
    "qw = Normal(loc=tf.Variable(tf.random_normal([D]) + .0),\n",
    "                        scale=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "\n",
    "inference = ed.KLqp({w: qw}, data={y: y_ph})\n",
    "inference.initialize(n_iter=5000, n_print=500)\n",
    "\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "for t in range(inference.n_iter):\n",
    "    X_batch, y_batch = next(data)\n",
    "    info_dict = inference.update(feed_dict={X: X_batch, y_ph: y_batch})\n",
    "    # info_dict = inference.update(feed_dict={X: X_train, y_ph: y_train})\n",
    "    inference.print_progress(info_dict)\n",
    "    if t == 1 or t % inference.n_print == 0:\n",
    "        # Check inferred posterior parameters.\n",
    "        mean, std = sess.run([qw.mean(), qw.stddev()])\n",
    "        print(\"\\nInferred mean & std:\")\n",
    "        print(mean)\n",
    "        print(std)\n",
    "        print('#### mse: %f\\ttest mse: %f\\n' % \n",
    "              (ed.evaluate('mean_squared_error', data={X: X_train, y: y_train}, n_samples=500),\n",
    "               ed.evaluate('mean_squared_error', data={X: X_test, y: y_test}, n_samples=500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 1s | Loss: 102.474\n",
      "#### mse: 1.640181\ttest mse: 1.853891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE v2.1\n",
    "qw = Normal(loc=tf.Variable(tf.random_normal([D]) + .0),\n",
    "                        scale=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "\n",
    "inference = ed.KLqp({w: qw}, data={y: y_train, X: X_train})\n",
    "inference.initialize(n_iter=5000, n_print=500)\n",
    "\n",
    "inference.run()\n",
    "\n",
    "print('#### mse: %f\\ttest mse: %f\\n' % \n",
    "      (ed.evaluate('mean_squared_error', data={X: X_train, y: y_train}, n_samples=500),\n",
    "       ed.evaluate('mean_squared_error', data={X: X_test, y: y_test}, n_samples=500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
